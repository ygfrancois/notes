攻击训练数据: 修改现有训练数据或者添加额外数据对训练过程造成影响
	由于算法中预测误差是以损失函数的凸点衡量的，这让攻击者有机会找到对推理表现影响最大的一组点进行攻击。
	防御：确保训练数据的保密性、一致性。
	
攻击开源库：机器学习的工作中会使用到很多的开源库，例如scikit-learn（含有很多机器学习模型API），tensorflow（谷歌的深度学习库）等，这些库中可能被埋下种子点。
	开源库的安全性较高，因为被很多人一起维护，特别是很大众的库，但之前也出现过python开源库被开发者埋下种子的例子出现过（被一个网友发现了），目前尚无很好的保证开源库安全性的方法出现。
	
攻击模型: 神经网络训练被外包给第三方时，被攻击者加入后门程序
	防御方法：加强AI模型整个供应链的安全防护
		考虑第三方的安全性
		在使用AI时，需要提高对AI训练模式的安全审计
		在传输过程中提供模型的完整性验证，存储时使加密存储来确保模型安全

对抗性攻击：创造出影响模型稳定性的对抗性数据
	图像：创建出人眼与AI模型识别结果不同，但人眼识别不出区别的对抗性数据（如攻击自动驾驶中车的感知）
	声音：创建出人耳无法辨识但模型会识别成某种指令的杂音（如攻击给车下发的指令）
	模型窃取：攻击者通过不断的输入和得到输出，获得模型的训练数据，从而训练出一个性能和该模型类似的模型，窃取该模型，从而针对性地攻击模型
	防御方法：
		模型保护：算法核心组件（包括网络、参数、用户模板数据、代码等）需要有安全保护机制；
		限制信息泄露：使模型给出的反馈信息尽可能少，让攻击者采集的信息不足以用于训练对抗网络；
		攻击者探测检测：通过攻击者探测系统的频率等信息设置检测系统，防止攻击者通过短时间的不断输入获取数据；
		集成学习：采用多种不同的检测机制提升系统的鲁棒性；
		增加人类交互认证，例如机器可以简单地发出一声警报、或请求输入音频验证码等方式。
		增强对抗性数据作为机器学习模型的输入的难度。例如语音识别系统可以使用声纹识别、音频滤波器等方式过滤掉大部分恶意语音。
		从机器学习模型本身训练其辨别良性、恶意数据的能力（在训练数据里加入对抗性数据）。
		开发异常检测系统
		PATE防御措施：将数据分区，训练多个模型，使用多个模型去综合做决策（类似于集成学习随机森林的思想）

